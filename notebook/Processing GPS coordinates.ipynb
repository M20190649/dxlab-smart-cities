{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and Visualizing GPS Coordinates with Spark\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective of this tutorial is to show how to use [Spark][1], a fast and general-purpose cluster computing system, for processing large volumes of GPS log files. For this purpose, you will work with a subset of the [Geolife GPS trajectory dataset][2]. For more information about the [Geolife project][3], visit the project web page.\n",
    "\n",
    "\n",
    "### Geolife trajectory dataset\n",
    "\n",
    "The Geolife trajectory dataset contains trajectories of 182 users over a period of 5 years (from April 2007 to August 2012). The dataset was collected by Microsoft Research Asia. Thus, most of the trajectories were done in China (but not only). The full Geolife dataset published by Microsoft is 3GB. For simplicity, you will only work with a sample of 150 MB.\n",
    "\n",
    "A Geolife trajectory is represented by **sequences of time-stamped points**. Each point (**GPS coordinate**) has the following information:\n",
    "\n",
    "    (latitude, longitude, ? , altitude, days, date, time)\n",
    "\n",
    "Trajectories are stored in GPS log files (`.plt`). Each log file contains all points of a trajectory. Log files are named using the trajectory starting time. All user trajectories of a single user are stored in folders named by the user ID.\n",
    "\n",
    "[1]: https://spark.apache.org/docs/latest/index.html\n",
    "[2]: https://www.microsoft.com/en-us/research/publication/geolife-gps-trajectory-dataset-user-guide/\n",
    "[3]: https://www.microsoft.com/en-us/research/project/geolife-building-social-networks-using-human-location-history/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Spark Cluster\n",
    "\n",
    "This notebook is pre-configured to connect to a **local spark cluster**. You can connect to the cluster by using `import pyspark`. The import instruction will create a `SparkContext` wich is your entry point to the cluster. The spark context is stored in the variable `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc   # Print Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that printing the spark context (`sc`) gives you information about the cluster:\n",
    "\n",
    "* **Spark Web user interface address** for monitoring the exeuction of spark programs ([Spark UI][1]). \n",
    "* **Number of cluster cores** (`n`) assigned to this notebook (`local[n]`). Note that the notebook represents a spark program named `Spark Notebook`.\n",
    "\n",
    "[1]: http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GPS Log Files\n",
    "\n",
    "In spark, you load files using `sc.textFile(path)`. In a real cluster, `path` points to a file or folder in a **distributed file system**. Because we are working on a local cluster, `path` points to the local file system. \n",
    "\n",
    "The following examples illustrate how to load:\n",
    "\n",
    "1. One specific log file (`20080618003409.plt`)\n",
    "2. All the logs of a specific user (```010```)\n",
    "3. All users logs (using wildcards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"     # For printing several outpus in 1 cell\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "rdd1 = sc.textFile( 'data/010/Trajectory/20080618003409.plt' )\n",
    "rdd2 = sc.textFile( 'data/010/Trajectory/*.plt' )\n",
    "rdd3 = sc.textFile( 'data/*/Trajectory/*.plt' )\n",
    "\n",
    "type(rdd1)\n",
    "type(rdd2)\n",
    "type(rdd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `sc.textFile()` returns [Resilient Distributed Datasets][1] (**RDDs**). By definition, RDDs variables (`rdd1`, `rdd2`, `rdd3`) do not contain any data. You can verify this by comparing **RDDs variables memory usage vs. file storage space**.\n",
    "\n",
    "[1]: https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#resilient-distributed-datasets-rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def sizeOf(rdd):\n",
    "    size = sys.getsizeof(rdd)\n",
    "    return str(size) + ' bytes'\n",
    "\n",
    "sizeOf(rdd1)  # data/010/Trajectory/20080618003409.plt = ~500 kb \n",
    "sizeOf(rdd2)  # data/010/Trajectory/*.plt              = ~39  mb\n",
    "sizeOf(rdd3)  # data/*/Trajectory/*.plt                = ~150 mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDDs physically reside inside the cluster**. The general idea is that you process RDD inside the cluster and only `rdd.collect()` collects the final (or partial) results from the cluster. \n",
    "\n",
    "The following examples illustrate how to collect the datasets associated to `rdd1`, `rdd2` and `rdd3`. For each RDD, the examples prints: \n",
    "\n",
    "* the type of the returned collections (i.e., list).\n",
    "* the memory usage of each collection.\n",
    "* the number of elements in the collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(list):\n",
    "    return str( len(list) ) + ' elements'\n",
    "\n",
    "list1 = rdd1.collect()    # data/010/Trajectory/20080618003409.plt = ~500 kb \n",
    "type(list1), sizeOf(list1), count(list1)\n",
    "\n",
    "list2 = rdd2.collect()    # data/010/Trajectory/*.plt = ~39  mb\n",
    "type(list2), sizeOf(list2), count(list2)\n",
    "\n",
    "list3 = rdd3.collect()    # data/*/Trajectory/*.plt  = ~150 mb\n",
    "type(list3), sizeOf(list3), count(list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving large RDDs out of the cluster (e.g., `rdd3`) consume time and computational resources. Because of this, Spark offers some convinient operations for collecting only parts of an RDD (see [RDD operations][1]). For instance:\n",
    "\n",
    "* `rdd.first()` return the first element in an RDD.\n",
    "* `rdd.take(n)` return a list containing the first ```n``` elements of an RDD.\n",
    "\n",
    "The following examples illustrate how to load the full dataset (~150 mb) but only collect and print the **first** and **three first** elements in the RDD. \n",
    "\n",
    "[1]: https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = sc.textFile( 'data/*/Trajectory/*.plt' )  # ~150 mb\n",
    "\n",
    "_first  = rdd3.first()\n",
    "_firstN = rdd3.take(3)\n",
    "\n",
    "type(_first ), sizeOf(_first ), _first\n",
    "type(_firstN), sizeOf(_firstN), _firstN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice how fast this operation was? This is due to the fact that **spark optimizes operations** and only loads in memory the data that is really required (e.g., the first 3 lines of the RDD \"containing\" the full data collection). You can verify this by looking at the statistics of the **last completed job** in the [Spark UI][2]. Look in particualr the job DAG (Directed Acyclic Graph).\n",
    "    \n",
    "[2]: http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Log Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Geolife log files contain: \n",
    "\n",
    "* Metadata (**lines 1-6**)\n",
    "* GPS coordinates `(latitude, longitude, ? , altitude, days, date, time)`\n",
    "\n",
    "You can verify this by loading and collecting the first `10` lines of a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile( 'data/010/Trajectory/20080618003409.plt' )\n",
    "rdd1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of removing the metadata is by using `RDD.filter(fn)` for filtering log lines not representing a GPS location. `RDD.filter(fn)` works as follows:\n",
    "\n",
    "1. Applies `fn` to every element `e` in RDD.\n",
    "2. Creates a new RDD containing all `e`'s where `fn(e) == True`.\n",
    "\n",
    "The following example illustrates the use of `RDD.filter(fn)` by defining a function called `notTraceMetadata(line)` for **keeping only the lines representing a GPS location**. The function works as follows: \n",
    "\n",
    "* Return `True` if a line can be comma-separated 7 times (`[lat, lon, ? , alt, days, date, time]`)\n",
    "* Return `False` otherwise.\n",
    "\n",
    "After `RDD.filter(fn)`, `linesRDD` contains only the lines representing a GPS coordinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notTraceMetadata(line):\n",
    "    a = line.split(\",\")\n",
    "    return True if len(a) == 7 else False\n",
    "\n",
    "linesRDD = rdd1.filter( notTraceMetadata )\n",
    "linesRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing GPS coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log lines contain several fields: `lat, lon, ? , alt, days, date, time`. Let us suppose that we are only interested in 3 of them: \n",
    "\n",
    "* Latitude\n",
    "* Longitude\n",
    "* Timestamp (date + time)\n",
    "\n",
    "We need thus to **parse** and **transform** every log line (`string`) into a new data structure conforming to this pattern. In Spark, this can be done by using the `rdd.map(fn)` operation. When invoked, the operation returns a new RDD containing the result of applying ```fn(e)``` to every elements in the original RDD. \n",
    "\n",
    "The following example illustrates the use of `rdd.map(fn)` by applying a function called `parseLogLine(line)` to `linesRDD`. The function receives as input a log line and produces as output a dictionary with the following structure:\n",
    "\n",
    "```json\n",
    "{ \n",
    "    \"lat\": float, \n",
    "    \"lon\": float, \n",
    "     \"ts\": float  // timestamp\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parseLogLine(line):\n",
    "    line = line.strip().split(\",\")\n",
    "    date = line[5] + \" \" + line[6]\n",
    "    return {\n",
    "        \"lat\": float( line[0] ), \n",
    "        \"lon\": float( line[1] ),\n",
    "        \"ts\" : datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "    }\n",
    "\n",
    "locationsRDD = linesRDD.map( parseLogLine )\n",
    "locationsRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing GPS Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cleaned the GPS log files, we can start visually exploring the geolife dataset using [Google Maps API][3] and [gmaps][4]. Before continuing, import and configure the library with your own [Google Maps API_KEY](https://developers.google.com/maps/documentation/javascript/get-api-key).\n",
    "\n",
    "[1]: http://jupyter-gmaps.readthedocs.io/en/latest/gmaps.html#base-maps\n",
    "[3]: https://developers.google.com/maps\n",
    "[4]: https://github.com/pbugnion/gmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmaps\n",
    "\n",
    "# Replace with your own API_KEY\n",
    "gmaps.configure(api_key=\"AIzaSyA__E_2UuLFdWKOgGZ42AECYZWY6Gp2y6U\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps\n",
    "\n",
    "Recall that log files represent **user trajectories**. One way of visualizing movement is by building a [heatmap][2]. Lets build one with gmaps. The general procedure is the following:\n",
    "\n",
    "1. Create a map\n",
    "2. Prepare the data to be visualized as a list of tuples (lat, lon)\n",
    "3. Create a layer (eg. a heatmap layer) using the data\n",
    "4. Add the layer to the map\n",
    "\n",
    "The following example illustrates the use of Spark for doing the heavy computations (transformation of step2) in Spark. \n",
    "\n",
    "[2]: https://en.wikipedia.org/wiki/Heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile( 'data/010/Trajectory/20080618003409.plt' )\n",
    "\n",
    "coordinatesRDD = rdd1.filter( notTraceMetadata ).map( parseLogLine ).map( \n",
    "    lambda cor: ( cor['lat'], cor['lon'] )\n",
    ")\n",
    "\n",
    "coordinatesRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is in the right format (**step 2**), you can collect it for building the heatmap layer (**step 3**) and add it to a gmap (**step 4**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_layer = gmaps.heatmap_layer( coordinatesRDD.collect() )\n",
    "\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking Trajectory Origin/Destination\n",
    "\n",
    "Recall that GPS log files represent single trajectories. It is thus possible to identify the **origin**/**destination** of a trajectory by identifying the first/final coordinate of a log file (i.e., the **coordinates having the min/max timestamps**).\n",
    "\n",
    "In spark, this can be done by:\n",
    "\n",
    "1. Projecting the timestamps\n",
    "2. Identyfing the min/max timestamps\n",
    "3. Finding the coordinate having the min/max timestamp (assuming there are no duplicates).\n",
    "\n",
    "The following example illustrates these steps for identyfing the origin of a trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile( 'data/010/Trajectory/20080618003409.plt' )\n",
    "\n",
    "coordinatesRDD = rdd1.filter( notTraceMetadata ).map( parseLogLine )\n",
    "\n",
    "# Step1. Projecting timestamps \n",
    "timestampsRDD = coordinatesRDD.map( lambda loc: loc['ts'] )\n",
    "timestampsRDD.take(5)\n",
    "\n",
    "# Step 2. Identifying MIN timestamp\n",
    "min_ts = timestampsRDD.min()\n",
    "min_ts\n",
    "\n",
    "# Step 3. Finding the coordinate with the MIN timestamp\n",
    "originRDD = coordinatesRDD.filter( lambda loc: loc['ts'] == min_ts )\n",
    "originRDD.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can build a map with a marker pointing the origin of a trajectory. The principle is the same as in the previous example. The only difference is that this time you will add a markers layer rather than a heatmap layer. If you feel lost check the [gmaps getting started guide][1].\n",
    "\n",
    "[1]: http://jupyter-gmaps.readthedocs.io/en/latest/gmaps.html#markers-and-symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to (lat, lon) and collect the result\n",
    "marker_locations = originRDD.map( \n",
    "    lambda cor: ( cor['lat'], cor['lon'] )\n",
    ").collect()\n",
    "\n",
    "print( marker_locations )\n",
    "\n",
    "fig = gmaps.figure()\n",
    "markers = gmaps.marker_layer(marker_locations)\n",
    "fig.add_layer(markers)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "1. Complete the previous example and identify the destination of a trajectory. Mark both origin and destination in a map.\n",
    "\n",
    "2. Explore different GPS log files and try to discover points of interest. Share your findings with your peers.\n",
    "\n",
    "3. Try the [rdd.union(rdd)][1] operation for visualizing more than 1 (sets of) GPS log files. Visit the [Spark UI][2] to observe the resulting graph. \n",
    "\n",
    "4. Explore the limits of the gmaps library. How many points can you visualize before the tool starts to be unresponsive?\n",
    "\n",
    "[1]: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n",
    "[2]: http://localhost:4040/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
